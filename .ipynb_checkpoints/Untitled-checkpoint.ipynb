{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로드\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 데이터 입력\n",
    "x = [10.5,24.6,1.6,18.3,19.5,10.5,21.3,24.1,0.3,19.9,21.5,2.3,16.7,22.2,8.5,3,5,17.9,9.2,19.3,21.3,20.3,19.5,10.5,14.4,17.5,22.1,5,9,21,14.9,15.3,0,4.8,6.3,19.4,4.5,1.4,15.1,20,17.5,21.1,18.3,15.7,12.7,19.5,15.2,10.5,8.5,24.7,12.6,16,20.9,3.8,24.2,15.6,20.9,21.3,12.4,4.1,5,18.4,18.9,1.9,2.1,18.4,11.4,14.8,20.2,18.7,11.9,23.9,14.6,13.2,23.6,10.4,2.1,3,14.5,23.5,19.4,12.4,0.4,21.6,15.1,4.6,22.6,14.3,19.7,20.5,4.5,4.6,7.1,1.3,8.3,0.2,22.9,14.2,8.4,24.7]\n",
    "y = [290.4,334.3,260.9,316.4,319.8,289.2,321.3,330.9,257.6,322.7,322.9,262.4,307.4,325.4,284.8,266.4,273.3,313.4,284.6,315.8,322.4,322.1,316.4,289.5,304.4,313.5,327.4,271.7,283.1,324.4,302.8,303.3,255.5,273,275.4,314.5,269.4,260,307,323,310.4,322.5,317,305.1,294.4,316.6,304.4,289.9,284.8,331.6,297.2,306.9,319.8,269.3,336,305.8,324.3,324.6,296,270,270.8,316.1,317.6,261.1,261.9,312.1,292.4,304.6,322.8,318.2,293.8,328.5,304.7,298.6,329.3,290.3,262.9,266.4,301.9,326.4,318,295.6,258.2,324.4,302.7,270,328.1,301.9,317.4,323.4,270.4,271.8,278.7,261.1,282.8,257.8,332.9,301,283,333.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:: \\:25.699180603027344 b:1.7987040281295776 Loss: 90417.3671875 \n",
      "100:: \\:15.779170989990234 b:33.446041107177734 Loss: 11209.291015625 \n",
      "200:: \\:14.191038131713867 b:61.51136016845703 Loss: 8566.0458984375 \n",
      "300:: \\:12.802773475646973 b:86.0446548461914 Loss: 6546.2412109375 \n",
      "400:: \\:11.589221000671387 b:107.49044799804688 Loss: 5002.83251953125 \n",
      "500:: \\:10.52839469909668 b:126.23726654052734 Loss: 3823.45751953125 \n",
      "600:: \\:9.601076126098633 b:142.624755859375 Loss: 2922.25439453125 \n",
      "700:: \\:8.790460586547852 b:156.9498748779297 Loss: 2233.612548828125 \n",
      "800:: \\:8.081863403320312 b:169.47215270996094 Loss: 1707.3956298828125 \n",
      "900:: \\:7.462437152862549 b:180.41859436035156 Loss: 1305.29052734375 \n",
      "1000:: \\:6.920974254608154 b:189.98727416992188 Loss: 998.0317993164062 \n",
      "1100:: \\:6.447651386260986 b:198.35179138183594 Loss: 763.241943359375 \n",
      "1200:: \\:6.033898830413818 b:205.66358947753906 Loss: 583.831298828125 \n",
      "1300:: \\:5.67221736907959 b:212.05519104003906 Loss: 446.7368469238281 \n",
      "1400:: \\:5.356051445007324 b:217.64244079589844 Loss: 341.9771728515625 \n",
      "1500:: \\:5.079675674438477 b:222.5265350341797 Loss: 261.9263916015625 \n",
      "1600:: \\:4.838082313537598 b:226.79595947265625 Loss: 200.7568359375 \n",
      "1700:: \\:4.626892566680908 b:230.528076171875 Loss: 154.01478576660156 \n",
      "1800:: \\:4.442282676696777 b:233.7904815673828 Loss: 118.29757690429688 \n",
      "1900:: \\:4.280906677246094 b:236.64230346679688 Loss: 91.00482177734375 \n",
      "2000:: \\:4.139837265014648 b:239.13526916503906 Loss: 70.14897155761719 \n",
      "2100:: \\:4.016522407531738 b:241.31448364257812 Loss: 54.2124137878418 \n",
      "2200:: \\:3.908726453781128 b:243.2194366455078 Loss: 42.034690856933594 \n",
      "2300:: \\:3.814497232437134 b:244.8846435546875 Loss: 32.72929000854492 \n",
      "2400:: \\:3.7321267127990723 b:246.34030151367188 Loss: 25.618616104125977 \n",
      "2500:: \\:3.6601226329803467 b:247.6127471923828 Loss: 20.185165405273438 \n",
      "2600:: \\:3.5971786975860596 b:248.72508239746094 Loss: 16.033092498779297 \n",
      "2700:: \\:3.542156457901001 b:249.6974334716797 Loss: 12.860418319702148 \n",
      "2800:: \\:3.4940602779388428 b:250.54739379882812 Loss: 10.436097145080566 \n",
      "2900:: \\:3.4520163536071777 b:251.29037475585938 Loss: 8.583589553833008 \n",
      "3000:: \\:3.415264129638672 b:251.9398651123047 Loss: 7.168020248413086 \n",
      "3100:: \\:3.3831329345703125 b:252.5076904296875 Loss: 6.086195468902588 \n",
      "3200:: \\:3.355048894882202 b:253.0039825439453 Loss: 5.2596659660339355 \n",
      "3300:: \\:3.330500364303589 b:253.43780517578125 Loss: 4.628091335296631 \n",
      "3400:: \\:3.309037208557129 b:253.8170928955078 Loss: 4.1454081535339355 \n",
      "3500:: \\:3.290281057357788 b:254.1485595703125 Loss: 3.7766811847686768 \n",
      "3600:: \\:3.273883104324341 b:254.43833923339844 Loss: 3.494899272918701 \n",
      "3700:: \\:3.2595486640930176 b:254.691650390625 Loss: 3.2795610427856445 \n",
      "3800:: \\:3.2470195293426514 b:254.91307067871094 Loss: 3.115046739578247 \n",
      "3900:: \\:3.236067533493042 b:255.1066131591797 Loss: 2.98931884765625 \n",
      "4000:: \\:3.2264928817749023 b:255.27581787109375 Loss: 2.893242835998535 \n",
      "4100:: \\:3.218123435974121 b:255.42372131347656 Loss: 2.8198325634002686 \n",
      "4200:: \\:3.2108078002929688 b:255.55299377441406 Loss: 2.763734817504883 \n",
      "4300:: \\:3.2044129371643066 b:255.66600036621094 Loss: 2.720872163772583 \n",
      "4400:: \\:3.198820114135742 b:255.7648468017578 Loss: 2.6880998611450195 \n",
      "4500:: \\:3.1939337253570557 b:255.8511962890625 Loss: 2.663074016571045 \n",
      "4600:: \\:3.1896610260009766 b:255.92669677734375 Loss: 2.643946886062622 \n",
      "4700:: \\:3.185926675796509 b:255.99269104003906 Loss: 2.629324436187744 \n",
      "4800:: \\:3.1826627254486084 b:256.0503845214844 Loss: 2.6181640625 \n",
      "4900:: \\:3.179805040359497 b:256.1008605957031 Loss: 2.6096127033233643 \n",
      "5000:: \\:3.177309274673462 b:256.14495849609375 Loss: 2.6030890941619873 \n",
      "5100:: \\:3.175140142440796 b:256.1833190917969 Loss: 2.5981333255767822 \n",
      "5200:: \\:3.1732311248779297 b:256.217041015625 Loss: 2.594325065612793 \n",
      "5300:: \\:3.171571731567383 b:256.2463684082031 Loss: 2.5914227962493896 \n",
      "5400:: \\:3.170116424560547 b:256.2720947265625 Loss: 2.5891964435577393 \n",
      "5500:: \\:3.1688454151153564 b:256.2945556640625 Loss: 2.5875022411346436 \n",
      "5600:: \\:3.167736530303955 b:256.31414794921875 Loss: 2.5862045288085938 \n",
      "5700:: \\:3.1667587757110596 b:256.3314208984375 Loss: 2.5852017402648926 \n",
      "5800:: \\:3.165896415710449 b:256.3466796875 Loss: 2.584439277648926 \n",
      "5900:: \\:3.1651790142059326 b:256.3593444824219 Loss: 2.583874225616455 \n",
      "6000:: \\:3.1644890308380127 b:256.3715515136719 Loss: 2.5834012031555176 \n",
      "6100:: \\:3.163968324661255 b:256.3807373046875 Loss: 2.5830886363983154 \n",
      "6200:: \\:3.1634504795074463 b:256.389892578125 Loss: 2.5828194618225098 \n",
      "6300:: \\:3.163019895553589 b:256.3974914550781 Loss: 2.5826234817504883 \n",
      "6400:: \\:3.162674903869629 b:256.4035949707031 Loss: 2.5824880599975586 \n",
      "6500:: \\:3.16232967376709 b:256.4096984863281 Loss: 2.582364082336426 \n",
      "6600:: \\:3.161984920501709 b:256.4158020019531 Loss: 2.5822579860687256 \n",
      "6700:: \\:3.1617748737335205 b:256.41949462890625 Loss: 2.5822081565856934 \n",
      "6800:: \\:3.161602258682251 b:256.42254638671875 Loss: 2.582160234451294 \n",
      "6900:: \\:3.1614296436309814 b:256.42559814453125 Loss: 2.5821216106414795 \n",
      "7000:: \\:3.161257028579712 b:256.42864990234375 Loss: 2.582089900970459 \n",
      "7100:: \\:3.1610848903656006 b:256.43170166015625 Loss: 2.5820627212524414 \n",
      "7200:: \\:3.160912275314331 b:256.43475341796875 Loss: 2.5820326805114746 \n",
      "7300:: \\:3.1607396602630615 b:256.43780517578125 Loss: 2.582014799118042 \n",
      "7400:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "7500:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "7600:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "7700:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "7800:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "7900:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8000:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8100:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8200:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8300:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8400:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8500:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8600:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8700:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8800:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "8900:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9000:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9100:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9200:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9300:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9400:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9500:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9600:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9700:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9800:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "9900:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n",
      "10000:: \\:3.1606228351593018 b:256.4398498535156 Loss: 2.5820047855377197 \n"
     ]
    }
   ],
   "source": [
    "# 선형 모델 클래스\n",
    "class LinearModel:\n",
    "    def __call__(self, x):\n",
    "        return self.Weight * x + self.Bias\n",
    "\n",
    "    def __init__(self):\n",
    "        self.Weight = tf.Variable(0.)\n",
    "        self.Bias = tf.Variable(0.)\n",
    "\n",
    "\n",
    "# 오차(손실) 구하기 함수\n",
    "def loss(y, pred):\n",
    "    return tf.reduce_mean(tf.square(y - pred))\n",
    "\n",
    "\n",
    "# 학습 함수\n",
    "def train(linear_model, x, y, lr):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(y, linear_model(x))\n",
    "\n",
    "    lr_weight, lr_bias = t.gradient(\n",
    "        current_loss, [linear_model.Weight, linear_model.Bias]\n",
    "    )\n",
    "    linear_model.Weight.assign_sub(lr * lr_weight)\n",
    "    linear_model.Bias.assign_sub(lr * lr_bias)\n",
    "\n",
    "\n",
    "# 학습 진행\n",
    "linear_model = LinearModel()\n",
    "epochs = 10000\n",
    "\n",
    "for epoch_count in range(epochs + 1):\n",
    "    real_loss = loss(y, linear_model(x))\n",
    "    train(linear_model, x, y, lr=0.003)\n",
    "    if epoch_count % 100 == 0:\n",
    "        print(f\"{epoch_count}:: \\:{linear_model.Weight.numpy()} b:{linear_model.Bias.numpy()} Loss: {real_loss.numpy()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
